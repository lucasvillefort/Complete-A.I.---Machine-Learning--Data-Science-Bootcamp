1) THE MOST MATTER LIBRALLY IN MACHINE LEARNING FOR USE:
    import numpy as np
    import pandas as pd
    import matplotlib.pyplot as plt
    import seaborn as sns
    from sklearn.model_selection import train_test_split # for splitting the data into training and testing sets
    from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, LabelEncoder, OneHotEncoder # for handling categorical data
    from sklearn.impute import SimpleImputer # for handling missing values
          example:
          imputer = SimpleImputer(missing_values=np.nan, strategy='mean/median/most_frequent/constant', fill_value=None)
    
    from sklearn.compose import ColumnTransformer # for applying transformers to columns of the dataset
          example: 
          ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough') # 0 is the index of the column to be transformed
          X = np.array(ct.fit_transform(X))

    
    from sklearn.pipeline import Pipeline # for creating a pipeline of workflows
          example:
          pipe = Pipeline(steps=[('imputer', SimpleImputer(strategy='mean')), ('scaler', StandardScaler())])
          X = pipe.fit_transform(X)
    
    # the best library for selecting the best features:
    from sklearn.feature_selection import SelectKBest, f_classif, f_regression, chi2, mutual_info_classif, mutual_info_regression
          example for regression:
          selector = SelectKBest(score_func=f_regression, k=5) # k is the number of features to be selected
          X_new = selector.fit_transform(X, y)

          example for classification:
          selector = SelectKBest(score_func=f_classif, k=5) # k is the number of features to be selected
          X_new = selector.fit_transform(X, y)

    # best metrics for evaluating the performance of a regression model:
    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score

          example:
          mse = mean_squared_error(y_test, y_pred) # the lower the better
          mae = mean_absolute_error(y_test, y_pred) # the lower the better
          r2 = r2_score(y_test, y_pred) # the higher the better
          evs = explained_variance_score(y_test, y_pred) # the higher the better
    
    # with cross-validation:
      from sklearn.model_selection import cross_val_score
              example:
              scores = cross_val_score(estimator, X, y, cv=10, scoring='accuracy') # cv is the number of folds
              print(np.mean(scores)*100)


    # best metrics for evaluating the performance of a classification model:
    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

          example:
          acc = accuracy_score(y_test, y_pred) # the higher the better
          prec = precision_score(y_test, y_pred) # the higher the better
          rec = recall_score(y_test, y_pred) # the higher the better
          f1 = f1_score(y_test, y_pred) # the higher the better
          roc_auc = roc_auc_score(y_test, y_pred) # the higher the better

          y_probs_positive = model.predict_proba(X_test)[:, 1]
          fpr, tpr, thresholds = roc_curve(y_test, y_probs_positive)
          # check the false positive rate and true positive rate
                  plt.plot(fpr, tpr)
                  plt.plot([0, 1], [0, 1], linestyle='--', color='red', label='guessing')
                  plt.xlabel('False Positive Rate')
                  plt.ylabel('True Positive Rate')
                  plt.title('ROC Curve')
                  plt.show()
            
          roc_auc = roc_auc_score(y_test, y_probs_positive) # the higher the better
      

      # confusion matrix:
      from sklearn.metrics import confusion_matrix
          cm = confusion_matrix(y_test, y_pred) # rows are the truth and columns are the predictions
          sns.heatmap(cm, annot=True)
          plt.xlabel('Predicted')
          plt.ylabel('Truth')
          plt.show()

          pd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)


      # tuning hyperparameters:
      
      param_distributions = {'n_estimators': [10, 100, 1000], 'max_depth': [None, 5, 10, 20, 30], 'max_features': ['auto', 'sqrt'], 'min_samples_split': [2, 4, 6], 'min_samples_leaf': [1, 2, 4]}

      from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
      grid = GridSearchCV(estimator, param_grid, scoring='accuracy', cv=10)
      grid.fit(X, y)
      print(grid.best_params_)
      print(grid.best_score_)

      random = RandomizedSearchCV(estimator, param_distributions, n_iter=100, scoring='accuracy', cv=10)
      random.fit(X, y)
      print(random.best_params_)
      print(random.best_score_)

      # save and loading a random model with joblib and pickle:
      import joblib
      joblib.dump(model, 'model.pkl')
      model = joblib.load('model.pkl')

      import pickle
      pickle.dump(model, open('model.pkl', 'wb'))
      model = pickle.load(open('model.pkl', 'rb'))
      
      




